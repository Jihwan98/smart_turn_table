{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QA0y8dMNMZGO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QA0y8dMNMZGO",
    "outputId": "e3103735-ef83-4704-f7e4-08d82965254a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chars2vec in c:\\users\\zxwlg\\miniconda3\\envs\\text\\lib\\site-packages (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install chars2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharing-memory",
   "metadata": {
    "id": "sharing-memory",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chars2vec\n",
    "\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import *\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "median-adelaide",
   "metadata": {
    "id": "median-adelaide"
   },
   "outputs": [],
   "source": [
    "spel_index = {' ':0, 'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,'o':15,'p':16,\\\n",
    "              'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automotive-pizza",
   "metadata": {
    "id": "automotive-pizza"
   },
   "outputs": [],
   "source": [
    "# txt 파일 읽기 (trigger word)\n",
    "\n",
    "r = open('./data/trigger_word.txt', mode='rt', encoding='utf-8')\n",
    "word = r.read()\n",
    "# word = \"friend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ignored-cutting",
   "metadata": {
    "id": "ignored-cutting"
   },
   "outputs": [],
   "source": [
    "# 알파벳 number indexing, 총 길이 15 zero padding, \n",
    "\n",
    "def indexing(lst_word):\n",
    "    if len(lst_word) < 16:\n",
    "        index_word = np.zeros(shape=(15,),dtype=int)\n",
    "        index_word = index_word.tolist()\n",
    "        j = 0\n",
    "        for i in lst_word:\n",
    "            index_word[j] = spel_index[i]\n",
    "            j += 1\n",
    "        return index_word\n",
    "    else:\n",
    "#         print(\"input under word length 15\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "technological-sellers",
   "metadata": {
    "id": "technological-sellers"
   },
   "outputs": [],
   "source": [
    "# indexing 해서 배열로 만들기\n",
    "\n",
    "def to_index(listed):\n",
    "    indexed = []\n",
    "    for i in listed:\n",
    "        if indexing(i) == None:\n",
    "            pass\n",
    "        else:\n",
    "            indexed.append(indexing(i))\n",
    "    indexed = np.array(indexed)\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swedish-appreciation",
   "metadata": {
    "id": "swedish-appreciation"
   },
   "outputs": [],
   "source": [
    "# 한자씩 빼기\n",
    "\n",
    "def one_miss(word):\n",
    "    word_list = []\n",
    "    for i in range(0, len(word)):\n",
    "        w = list(word)\n",
    "        w.pop(i)\n",
    "        w.append(\" \")\n",
    "        word_list.append(w)\n",
    "    word_list = np.array(word_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "circular-frank",
   "metadata": {
    "id": "circular-frank"
   },
   "outputs": [],
   "source": [
    "# 두자씩 빼기\n",
    "\n",
    "def two_miss(word):\n",
    "    word_list = []\n",
    "    r = int(len(word) * (len(word) - 1) / 2)    # nC2\n",
    "    \n",
    "    while len(word_list) < r:\n",
    "        w = list(word)\n",
    "        a = randrange(0, len(word))\n",
    "        b = randrange(0, len(word) - 1)\n",
    "        w.pop(a)\n",
    "        w.pop(b)\n",
    "        if w not in word_list:    \n",
    "            word_list.append(w)\n",
    "    word_list = np.array(word_list)\n",
    "            \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passing-cruise",
   "metadata": {
    "id": "passing-cruise"
   },
   "outputs": [],
   "source": [
    "# 기본 단어 + one_miss()\n",
    "\n",
    "def default_word(word):\n",
    "    word_list = np.array([list(word)])\n",
    "    word_list = np.vstack((word_list, one_miss(word)))\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "injured-deadline",
   "metadata": {
    "id": "injured-deadline"
   },
   "outputs": [],
   "source": [
    "# 랜덤하게 한자 추가하는 노이즈\n",
    "\n",
    "def make_noise(word_list):\n",
    "    a = word_list.tolist()\n",
    "    \n",
    "    for i in range(0, len(a)):\n",
    "        x = randrange(0, len(a[0]))\n",
    "        y = randrange(0, 27)\n",
    "\n",
    "        a[i].insert(x, list(spel_index.keys())[y])\n",
    "    a = np.array(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleased-enlargement",
   "metadata": {
    "id": "pleased-enlargement"
   },
   "outputs": [],
   "source": [
    "# change spelling\n",
    "\n",
    "def change_spel(data, spel_from, spel_to):\n",
    "    changed = np.where(data == spel_index[spel_from], spel_index[spel_to], data)\n",
    "    data = np.append(data, changed, axis=0)\n",
    "    data = np.unique(data, axis=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interim-digit",
   "metadata": {
    "id": "interim-digit"
   },
   "outputs": [],
   "source": [
    "def make_true_data(word):\n",
    "    default_list = default_word(word)  # default\n",
    "    noise_word = make_noise(default_list)  # default 에서 노이즈\n",
    "    \n",
    "    # indexing\n",
    "    indexed_d = to_index(default_list)  \n",
    "    indexed_n = to_index(noise_word)\n",
    "    if len(word) < 6:\n",
    "        d_t_data = indexed_d\n",
    "    else:\n",
    "        two_miss_word = two_miss(word)  # 두자 빼기\n",
    "        indexed_t = to_index(two_miss_word)\n",
    "        d_t_data = np.vstack((indexed_d, indexed_t))\n",
    "        \n",
    "    #  모음 변환 a (1), e (5), i(9), o(15), u(21)\n",
    "    change_a_e = change_spel(d_t_data, \"a\", \"e\")\n",
    "    change_a_i = change_spel(d_t_data, \"a\", \"i\")\n",
    "    change_a_o = change_spel(d_t_data, \"a\", \"o\")\n",
    "    change_a_u = change_spel(d_t_data, \"a\", \"u\")\n",
    "    change_a = np.vstack((change_a_e, change_a_i, change_a_o, change_a_u))\n",
    "\n",
    "    change_e_a = change_spel(d_t_data, \"e\", \"a\")\n",
    "    change_e_i = change_spel(d_t_data, \"e\", \"i\")\n",
    "    change_e_o = change_spel(d_t_data, \"e\", \"o\")\n",
    "    change_e_u = change_spel(d_t_data, \"e\", \"u\")\n",
    "    change_e = np.vstack((change_e_a, change_e_i, change_e_o, change_e_u))\n",
    "    \n",
    "    change_i_a = change_spel(d_t_data, \"i\", \"a\")\n",
    "    change_i_e = change_spel(d_t_data, \"i\", \"e\")\n",
    "    change_i_o = change_spel(d_t_data, \"i\", \"o\")\n",
    "    change_i_u = change_spel(d_t_data, \"i\", \"u\")\n",
    "    change_i = np.vstack((change_i_a, change_i_e, change_i_o, change_i_u))\n",
    "\n",
    "    change_o_a = change_spel(d_t_data, \"o\", \"a\")\n",
    "    change_o_e = change_spel(d_t_data, \"o\", \"e\")\n",
    "    change_o_i = change_spel(d_t_data, \"o\", \"i\")\n",
    "    change_o_u = change_spel(d_t_data, \"o\", \"u\")\n",
    "    change_o = np.vstack((change_o_a, change_o_e, change_o_i, change_o_u))\n",
    "\n",
    "    change_u_a = change_spel(d_t_data, \"u\", \"a\")\n",
    "    change_u_e = change_spel(d_t_data, \"u\", \"e\")\n",
    "    change_u_i = change_spel(d_t_data, \"u\", \"i\")\n",
    "    change_u_o = change_spel(d_t_data, \"u\", \"o\")\n",
    "    change_u = np.vstack((change_u_a, change_u_e, change_u_i, change_u_o))\n",
    "    \n",
    "    d_t_data = np.vstack((change_a, change_e, change_i, change_o, change_u))\n",
    "    d_t_data = np.unique(d_t_data, axis=0)\n",
    "    \n",
    "    # 합치기\n",
    "    true_data = np.vstack((d_t_data, indexed_n))\n",
    "    \n",
    "    # label 넣어주기\n",
    "    true_label = np.ones(shape=(true_data.shape[0],1),dtype=int)\n",
    "    true_data = np.hstack((true_data, true_label))\n",
    "    \n",
    "    return true_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consecutive-sunset",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "consecutive-sunset",
    "outputId": "cfb90053-8403-4a7b-ccd7-5210138bc1d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 16)\n"
     ]
    }
   ],
   "source": [
    "true_data_p = make_true_data(word)\n",
    "print(true_data_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unlikely-reception",
   "metadata": {
    "id": "unlikely-reception"
   },
   "outputs": [],
   "source": [
    "def to_list(text_word):\n",
    "    listed = []\n",
    "    j = 0\n",
    "    for i in text_word:\n",
    "        listed.append(list(i))\n",
    "    return listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "greatest-improvement",
   "metadata": {
    "id": "greatest-improvement"
   },
   "outputs": [],
   "source": [
    "# false data\n",
    "\n",
    "false = pd.read_csv(\"./data/others.txt\",sep='\\n', names=[\"word\"])\n",
    "\n",
    "def false_data(data):\n",
    "    data.word = data.word.str.lower()\n",
    "    if word in data.word.values:\n",
    "        f_index = data.word[data.word == word].index[0]\n",
    "        false_word = data.word.drop(f_index)\n",
    "    else:\n",
    "        false_word = data.word\n",
    "    return false_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "annoying-appraisal",
   "metadata": {
    "id": "annoying-appraisal"
   },
   "outputs": [],
   "source": [
    "false_word = false_data(false)\n",
    "\n",
    "false_data_indexed = to_index(to_list(false_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "seven-setup",
   "metadata": {
    "id": "seven-setup"
   },
   "outputs": [],
   "source": [
    "# True 데이터에서 생성된 데이터 중 False 데이터에 있는 것 삭제\n",
    "\n",
    "false_data_word = false_data_indexed.tolist()\n",
    "true_data_word = np.delete(true_data_p, -1, 1).tolist()\n",
    "\n",
    "for i in true_data_word:\n",
    "    if i in false_data_word:\n",
    "        d_index = false_data_word.index(i)\n",
    "        false_data_word.pop(d_index)\n",
    "        \n",
    "false_data_word = np.array(false_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-multiple",
   "metadata": {
    "id": "average-multiple"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "signal-routine",
   "metadata": {
    "id": "signal-routine"
   },
   "outputs": [],
   "source": [
    "def delete_blank(word):  # input : list 형태\n",
    "    w = []\n",
    "    for i in range(len(word)):\n",
    "        w.append(word[i].rstrip())\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "proper-collector",
   "metadata": {
    "id": "proper-collector"
   },
   "outputs": [],
   "source": [
    "def index_spel(data):\n",
    "    spel_index_s = Series(spel_index, dtype=np.int)\n",
    "    list_data = spel_index_s.index\n",
    "    list_name = spel_index_s.values\n",
    "    num_to_spel = Series(data = list_data, index = list_name)\n",
    "    \n",
    "    data_l = data.tolist()\n",
    "    \n",
    "    for i in data_l:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = num_to_spel[i[j]]\n",
    "    spel_data = []\n",
    "    for j in data_l:\n",
    "        spel_data.append(''.join(j))\n",
    "    \n",
    "    spel_data = delete_blank(spel_data)\n",
    "    return spel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unavailable-calibration",
   "metadata": {
    "id": "unavailable-calibration"
   },
   "outputs": [],
   "source": [
    "w_t = index_spel(np.delete(true_data_p,-1,1))\n",
    "w_f = index_spel(false_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cooperative-mission",
   "metadata": {
    "id": "cooperative-mission"
   },
   "outputs": [],
   "source": [
    "c2v_model = chars2vec.load_model('eng_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "corporate-behalf",
   "metadata": {
    "id": "corporate-behalf"
   },
   "outputs": [],
   "source": [
    "true_word_embeddings = c2v_model.vectorize_words(w_t)\n",
    "false_word_embeddings = c2v_model.vectorize_words(w_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "separated-wholesale",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "separated-wholesale",
    "outputId": "d4c46220-4348-4035-e355-f22d7a9c56d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 51)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label = np.ones(shape=(true_word_embeddings.shape[0],1),dtype=int)\n",
    "true_data = np.hstack((true_word_embeddings, true_label))\n",
    "\n",
    "true_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "freelance-bikini",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "freelance-bikini",
    "outputId": "4893f318-0da1-40fc-be51-a8d6f1025f49",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2989, 51)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_label = np.zeros(shape=(false_word_embeddings.shape[0],1),dtype=int)\n",
    "false_data = np.hstack((false_word_embeddings, false_label))\n",
    "\n",
    "false_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mineral-catalyst",
   "metadata": {
    "id": "mineral-catalyst"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(true_data)\n",
    "train_true = true_data[:-20]\n",
    "test_true = true_data[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fifth-mailing",
   "metadata": {
    "id": "fifth-mailing"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(false_data)\n",
    "train_false = false_data[:-1000]\n",
    "test_false = false_data[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "focused-slave",
   "metadata": {
    "id": "focused-slave"
   },
   "outputs": [],
   "source": [
    "train_data = np.vstack((train_true, train_false))\n",
    "test_data = np.vstack((test_true, test_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noted-montgomery",
   "metadata": {
    "id": "noted-montgomery"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "convinced-onion",
   "metadata": {
    "id": "convinced-onion"
   },
   "outputs": [],
   "source": [
    "X_train = np.delete(train_data,-1,1)\n",
    "y_train = train_data[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "purple-flour",
   "metadata": {
    "id": "purple-flour"
   },
   "outputs": [],
   "source": [
    "X_test = np.delete(test_data,-1,1)\n",
    "y_test = test_data[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "smoking-arkansas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smoking-arkansas",
    "outputId": "363083ba-aa4a-4dda-d8f3-299a2756b32d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxwlg\\miniconda3\\envs\\text\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=4, gamma=0.05)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=4, gamma=0.05)\n",
    " \n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "absent-tension",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "absent-tension",
    "outputId": "5217a97c-8052-408a-c52b-a2a688a448e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "normal-photograph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "normal-photograph",
    "outputId": "4d8c7b2b-4484-43b2-8393-ed7bf4291388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    0]\n",
      " [   0   20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "cf = confusion_matrix(y_test, y_test_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "consistent-median",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "consistent-median",
    "outputId": "9c0e1ed4-3b54-4273-d640-b35a11eab6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1988    1]\n",
      " [   0  112]]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = svm_model.predict(X_train)\n",
    "cf = confusion_matrix(y_train, y_train_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "referenced-newfoundland",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "referenced-newfoundland",
    "outputId": "64b98a71-7ff2-43eb-f30f-7135f3ea393d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(['friend'])\n",
    "a = c2v_model.vectorize_words(a)\n",
    "\n",
    "test_svm = svm_model.predict(a)\n",
    "test_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-arthritis",
   "metadata": {
    "id": "uniform-arthritis"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "chars2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
